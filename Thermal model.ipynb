{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e38a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "#from dataset import Dataset, collate_fn\n",
    "SEED = 42\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Wild_Atomic_Coordinate.npy',W_Structural)\n",
    "np.save('Mutant_Atomic_Coordinate.npy',M_Structural)\n",
    "np.save('Wild_Node.npy',W_Node)\n",
    "np.save('Mutant_Node.npy',M_Node)\n",
    "np.save('Wild_Edge_Contact.npy',W_Edge_Contact)\n",
    "np.save('Mutant_Edge_Contact.npy',M_Edge_Contact)\n",
    "np.save('Wild_Edge_Distance.npy',W_Edge_Distance)\n",
    "np.save('Mutant_Edge_Distance.npy',M_Edge_Distance)\n",
    "np.save('DGG.npy',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e33959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atomic_wild=np.load('Wild_Atomic_Coordinate.npy',allow_pickle=True)\n",
    "Atomic=np.load('Mutant_Atomic_Coordinate.npy',allow_pickle=True)\n",
    "sequence_features_wild=np.load('Wild_Node.npy',allow_pickle=True)\n",
    "sequence_features=np.load('Mutant_Node.npy',allow_pickle=True)\n",
    "sequence_graphs_wild=np.load('Wild_Edge_contact.npy',allow_pickle=True)\n",
    "sequence_graphs=np.load('Mutant_Edge_Contact.npy',allow_pickle=True)\n",
    "EdgeDW=np.load('Wild_Edge_Distance.npy',allow_pickle=True)\n",
    "EdgeDM=np.load('Mutant_Edge_Distance.npy',allow_pickle=True)\n",
    "#Dihedral = np.load('train_torsion_final_mutant.npy',allow_pickle=True)\n",
    "#Dihedral_wild = np.load('train_torsion_final_wild.npy',allow_pickle=True)\n",
    "labels=np.load('DGG.npy',allow_pickle=True)\n",
    "sequence_names=np.load('Seq_Name.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "026e5ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2530"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4398cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atomic_wild=np.load('Wild_Atomic_Coordinate.npy',allow_pickle=True)\n",
    "Atomic=np.load('Mutant_Atomic_Coordinate.npy',allow_pickle=True)\n",
    "sequence_features_wild=np.load('Wild_Node.npy',allow_pickle=True)\n",
    "sequence_features=np.load('Mutant_Node.npy',allow_pickle=True)\n",
    "sequence_graphs_wild=np.load('Wild_Edge_Contact.npy',allow_pickle=True)\n",
    "sequence_graphs=np.load('Mutant_Edge_Contact.npy',allow_pickle=True)\n",
    "EdgeDW=np.load('Wild_Edge_Distance.npy',allow_pickle=True)\n",
    "EdgeDM=np.load('Mutant_Edge_Distance.npy',allow_pickle=True)\n",
    "Dihedral = np.load('Dih.npy',allow_pickle=True)\n",
    "Dihedral_wild = np.load('Dih_w.npy',allow_pickle=True)\n",
    "labels=np.load('dgg.npy',allow_pickle=True)\n",
    "sequence_names=np.load('Seq_Name.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ff5124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Path = './Model/'\n",
    "Result_Path = './Result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d18ca3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dihedral_wild = np.array(Dihedral_wild)\n",
    "Dihedral = np.array(Dihedral)\n",
    "Atomic_wild = np.array(Atomic_wild)\n",
    "Atomic = np.array(Atomic)\n",
    "sequence_features =np.array(sequence_features)\n",
    "sequence_features_wild =np.array(sequence_features_wild)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8c9acad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.09000015e-01,  1.64000005e-01,  2.60000005e-02, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 9.07999992e-01,  8.60000029e-02,  7.00000022e-03, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 7.45999992e-01,  1.86000004e-01,  6.89999983e-02, ...,\n",
       "          7.40000000e+00,  1.75700000e+01,  1.74000000e+00],\n",
       "        ...,\n",
       "        [ 8.10999990e-01,  1.50999993e-01,  3.70000005e-02, ...,\n",
       "          1.10000000e+01,  1.81700000e+01, -6.04000000e+00],\n",
       "        [ 4.37000006e-01,  4.11000013e-01,  1.51999995e-01, ...,\n",
       "          8.80000000e+00,  1.79900000e+01, -1.01000000e+00],\n",
       "        [ 2.80999988e-01,  3.81000012e-01,  3.38999987e-01, ...,\n",
       "          6.30000000e+00,  1.79300000e+01,  1.52000000e+00]],\n",
       "\n",
       "       [[ 6.21999979e-01,  2.73999989e-01,  1.04000002e-01, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 8.06999981e-01,  1.58999994e-01,  3.29999998e-02, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 8.41000021e-01,  1.44999996e-01,  1.40000004e-02, ...,\n",
       "          1.10000000e+01,  1.81700000e+01, -6.04000000e+00],\n",
       "        ...,\n",
       "        [ 6.60000026e-01,  2.87000000e-01,  5.29999994e-02, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 1.65999994e-01,  3.35000008e-01,  4.99000013e-01, ...,\n",
       "          8.80000000e+00,  1.81600000e+01,  2.30000000e+00],\n",
       "        [ 1.73999995e-01,  5.18999994e-01,  3.07000011e-01, ...,\n",
       "          8.80000000e+00,  1.81600000e+01,  2.30000000e+00]],\n",
       "\n",
       "       [[ 7.45999992e-01,  1.86000004e-01,  6.89999983e-02, ...,\n",
       "          7.40000000e+00,  1.75700000e+01,  1.74000000e+00],\n",
       "        [ 6.21999979e-01,  2.73999989e-01,  1.04000002e-01, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 8.06999981e-01,  1.58999994e-01,  3.29999998e-02, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        ...,\n",
       "        [ 2.80999988e-01,  3.81000012e-01,  3.38999987e-01, ...,\n",
       "          6.30000000e+00,  1.79300000e+01,  1.52000000e+00],\n",
       "        [ 6.60000026e-01,  2.87000000e-01,  5.29999994e-02, ...,\n",
       "          1.12000000e+01,  1.73000000e+01, -4.65000000e+00],\n",
       "        [ 1.65999994e-01,  3.35000008e-01,  4.99000013e-01, ...,\n",
       "          8.80000000e+00,  1.81600000e+01,  2.30000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.60000008e-02,  1.65000007e-01,  8.19000006e-01, ...,\n",
       "          7.90000000e+00,  1.79600000e+01,  3.88000000e+00],\n",
       "        [ 1.49000004e-01,  4.23999995e-01,  4.28000003e-01, ...,\n",
       "          8.50000000e+00,  1.73600000e+01,  1.36000000e+00],\n",
       "        [ 3.79000008e-01,  5.09000003e-01,  1.11000001e-01, ...,\n",
       "          1.50000000e+01,  1.86000000e+01, -4.68000000e+00],\n",
       "        ...,\n",
       "        [ 7.58000016e-01,  2.17999995e-01,  2.40000002e-02, ...,\n",
       "          8.50000000e+00,  1.80800000e+01, -2.10000000e-01],\n",
       "        [ 9.08999979e-01,  8.39999989e-02,  7.00000022e-03, ...,\n",
       "          1.68000000e+01,  1.86200000e+01, -4.81000000e+00],\n",
       "        [ 5.60000017e-02,  5.49000025e-01,  3.95000011e-01, ...,\n",
       "          8.20000000e+00,  1.74700000e+01,  9.60000000e-01]],\n",
       "\n",
       "       [[ 1.60000008e-02,  1.65000007e-01,  8.19000006e-01, ...,\n",
       "          7.90000000e+00,  1.79600000e+01,  3.88000000e+00],\n",
       "        [ 1.49000004e-01,  4.23999995e-01,  4.28000003e-01, ...,\n",
       "          8.50000000e+00,  1.73600000e+01,  1.36000000e+00],\n",
       "        [ 3.79000008e-01,  5.09000003e-01,  1.11000001e-01, ...,\n",
       "          1.50000000e+01,  1.86000000e+01, -4.68000000e+00],\n",
       "        ...,\n",
       "        [ 7.58000016e-01,  2.17999995e-01,  2.40000002e-02, ...,\n",
       "          8.50000000e+00,  1.80800000e+01, -2.10000000e-01],\n",
       "        [ 9.08999979e-01,  8.39999989e-02,  7.00000022e-03, ...,\n",
       "          1.68000000e+01,  1.86200000e+01, -4.81000000e+00],\n",
       "        [ 5.60000017e-02,  5.49000025e-01,  3.95000011e-01, ...,\n",
       "          8.20000000e+00,  1.74700000e+01,  9.60000000e-01]],\n",
       "\n",
       "       [[ 1.60000008e-02,  1.65000007e-01,  8.19000006e-01, ...,\n",
       "          7.90000000e+00,  1.79600000e+01,  3.88000000e+00],\n",
       "        [ 1.49000004e-01,  4.23999995e-01,  4.28000003e-01, ...,\n",
       "          8.50000000e+00,  1.73600000e+01,  1.36000000e+00],\n",
       "        [ 3.79000008e-01,  5.09000003e-01,  1.11000001e-01, ...,\n",
       "          1.50000000e+01,  1.86000000e+01, -4.68000000e+00],\n",
       "        ...,\n",
       "        [ 7.58000016e-01,  2.17999995e-01,  2.40000002e-02, ...,\n",
       "          8.50000000e+00,  1.80800000e+01, -2.10000000e-01],\n",
       "        [ 9.08999979e-01,  8.39999989e-02,  7.00000022e-03, ...,\n",
       "          1.68000000e+01,  1.86200000e+01, -4.81000000e+00],\n",
       "        [ 5.60000017e-02,  5.49000025e-01,  3.95000011e-01, ...,\n",
       "          8.20000000e+00,  1.74700000e+01,  9.60000000e-01]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Atomic_wild.astype(np.float64)\n",
    "Dihedral.astype(np.float64)\n",
    "Dihedral_wild.astype(np.float64)\n",
    "Atomic.astype(np.float64)\n",
    "sequence_features.astype(np.float64)\n",
    "sequence_features_wild.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a34ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "Aw = []\n",
    "\n",
    "# Iterate over the range\n",
    "for i in range(len(Dihedral)):\n",
    "    #print(f\"Shapes before concatenation: Atomic[{i}]: {Atomic[i].shape}, Dihedral[{i}]: {Dihedral[i].shape}\")\n",
    "    # Concatenate and append to A\n",
    "    A.append(np.concatenate(( Atomic[i],Dihedral[i]), axis=1))\n",
    "    \n",
    "    #print(f\"Shape after concatenation: A[{i}]: {A[i].shape}\")\n",
    "\n",
    "    # Repeat the process for Aw\n",
    "    #print(f\"Shapes before concatenation: Atomic_wild[{i}]: {Atomic_wild[i].shape}, Dihedral_wild[{i}]: {Dihedral_wild[i].shape}\")\n",
    "    Aw.append(np.concatenate(( Atomic_wild[i],Dihedral_wild[i]), axis=1))\n",
    "    \n",
    "    #print(f\"Shape after concatenation: Aw[{i}]: {Aw[i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "64f243a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Atomic\n",
    "Aw = Atomic_wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ac390dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 71)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11794d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "zipped = list(zip(sequence_names,labels))\n",
    "ds = pd.DataFrame(zipped, columns=['names','stability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8173ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('intro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69232174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit using the train set\n",
    "#scaler.fit(M)\n",
    "# transform the test test\n",
    "#X_scaled = scaler.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "27552afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "WG = dict(zip(sequence_names, sequence_graphs_wild))\n",
    "MG = dict(zip(sequence_names, sequence_graphs))\n",
    "WF=dict(zip(sequence_names, sequence_features_wild))\n",
    "MF=dict(zip(sequence_names, sequence_features))\n",
    "WS=dict(zip(sequence_names, Aw))\n",
    "MS=dict(zip(sequence_names, A))\n",
    "zipped = list(zip(sequence_names, labels))\n",
    "ds = pd.DataFrame(zipped, columns=['names', 'stability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "afa6878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NodeM(name):\n",
    "    return MF[name]\n",
    "def NodeW(name):\n",
    "    return WF[name]\n",
    "def GraphM(name):\n",
    "    return MG[name]\n",
    "def GraphW(name):\n",
    "    return WG[name]\n",
    "def StM(name):\n",
    "    return MS[name]\n",
    "def StW(name):\n",
    "    return WS[name]\n",
    "def DM(name):\n",
    "    return MD[name]\n",
    "def DW(name):\n",
    "    return WD[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2471549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ds['names'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e2f188a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "  \n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.names = dataframe['names'].values.tolist()\n",
    "        self.labels = dataframe['stability'].values.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sequence_name = self.names[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        sequence_feature = NodeM(sequence_name)\n",
    "        scaler.fit(sequence_feature)\n",
    "        sequence_feature = scaler.transform(sequence_feature)\n",
    "        # L * L\n",
    "        sequence_graph = GraphM(sequence_name)\n",
    "        \n",
    "        A= StM(sequence_name)\n",
    "        scaler.fit(A)\n",
    "        A = scaler.transform(A)\n",
    "\n",
    "        # L * 91\n",
    "        sequence_feature_wild = NodeW(sequence_name)\n",
    "        scaler.fit(sequence_feature)\n",
    "        # L * L\n",
    "        sequence_feature_wild = scaler.transform(sequence_feature_wild)\n",
    "\n",
    "        sequence_graph_wild =  GraphW(sequence_name)\n",
    "        \n",
    "        Aw=  StW(sequence_name)\n",
    "        scaler.fit(Aw)\n",
    "        Aw = scaler.transform(Aw)\n",
    "\n",
    "        sample = {'sequence_feature': sequence_feature,\\\n",
    "                  'sequence_feature_wild': sequence_feature_wild,\\\n",
    "                  'sequence_graph': sequence_graph, \\\n",
    "                  'sequence_graph_wild': sequence_graph_wild, \\\n",
    "                  'A': A,\\\n",
    "                  'Aw': Aw,\\\n",
    "                  'label': label, \\\n",
    "                  'sequence_name': sequence_name, \\\n",
    "                  }\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_natoms_m = 11\n",
    "    max_natoms_w = 11\n",
    "    sequence_feature = np.zeros((len(batch), max_natoms_m, 71))\n",
    "    sequence_feature_wild = np.zeros((len(batch), max_natoms_w, 71))\n",
    "    sequence_graph = np.zeros((len(batch), max_natoms_m, max_natoms_m))\n",
    "    sequence_graph_wild = np.zeros((len(batch), max_natoms_w, max_natoms_w))\n",
    "    A = np.zeros((len(batch), max_natoms_m, 35))\n",
    "    Aw = np.zeros((len(batch), max_natoms_w,35))\n",
    "\n",
    "    sequence_names = [] \n",
    "    labels=[]   \n",
    "    for i in range(len(batch)):\n",
    "        natom1 = len(batch[i]['sequence_feature'])\n",
    "        natom2 = len(batch[i]['sequence_feature_wild'])\n",
    "        natom3 = len(batch[i]['A'])\n",
    "        natom4 = len(batch[i]['Aw'])\n",
    "        sequence_feature[i,:natom1] = batch[i]['sequence_feature']\n",
    "        sequence_feature_wild[i,:natom2] = batch[i]['sequence_feature_wild']\n",
    "        sequence_graph[i,:natom1,:natom1] = batch[i]['sequence_graph']\n",
    "        sequence_graph_wild[i,:natom2,:natom2] = batch[i]['sequence_graph_wild']\n",
    "        A[i,:natom1,:35] = batch[i]['A']\n",
    "        Aw[i,:natom2,:35] = batch[i]['Aw']\n",
    "        sequence_names.append(batch[i]['sequence_name'])\n",
    "        labels.append(batch[i]['label'])\n",
    "        labels= np.asarray(labels)\n",
    "    sequence_feature= torch.from_numpy(sequence_feature).float()\n",
    "    sequence_feature_wild = torch.from_numpy(sequence_feature_wild).float()\n",
    "    sequence_graph = torch.from_numpy(sequence_graph).float()\n",
    "    sequence_graph_wild = torch.from_numpy(sequence_graph_wild).float()\n",
    "    A = torch.from_numpy(A).float()\n",
    "    Aw = torch.from_numpy(Aw).float()\n",
    "    labels= torch.from_numpy(labels).float()\n",
    "\n",
    "    return sequence_feature, sequence_feature_wild,sequence_graph , sequence_graph_wild,A,Aw, labels, sequence_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8bb9fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "NUMBER_EPOCHS = 1000\n",
    "LEARNING_RATE = 5E-4\n",
    "WEIGHT_DECAY = 1E-7\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# GCN parameters\n",
    "GCN_FEATURE_DIM = 71\n",
    "GCN_HIDDEN_DIM1 = 256\n",
    "GCN_HIDDEN_DIM2 = 128\n",
    "# Increased hidden dimension\n",
    "GCN_OUTPUT_DIM = 64\n",
    "DROPOUT_RATE = 0.5 \n",
    "GCN_HIDDEN_DIM = 256\n",
    "# Attention parameters\n",
    "DENSE_DIM = 16\n",
    "ATTENTION_HEADS = 4\n",
    "GAT_FEATURE_DIM = 71\n",
    "GAT_HIDDEN_DIM = 256\n",
    "GAT_OUTPUT_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "DROPOUT_RATE = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2d1761a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.5):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear(x)\n",
    "        x = torch.matmul(adj, x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ComplexGraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_features, num_layers=2, dropout_rate=0.5):\n",
    "        super(ComplexGraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConvolution(input_features, hidden_features, dropout_rate)\n",
    "        self.convs = nn.ModuleList([GraphConvolution(hidden_features, hidden_features, dropout_rate) for _ in range(num_layers - 1)])\n",
    "        self.conv_final = GraphConvolution(hidden_features, output_features, dropout_rate)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, 11 * output_features)  # Adjusted for the desired output size\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, adj)\n",
    "        x = self.conv_final(x, adj)\n",
    "\n",
    "        # Global pooling (you can use other pooling strategies based on your task)\n",
    "        x = torch.mean(x, dim=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Reshape to the desired output size (11 x output_features)\n",
    "        x = x.view(11, -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have node features (x) and adjacency matrix (adj)\n",
    "input_features = 71\n",
    "hidden_features = 64\n",
    "output_features = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dad7f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = input @ self.weight    # X * W\n",
    "        output = adj @ support           # A * X * W\n",
    "        if self.bias is not None:        # A * X * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0252f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(GCN_FEATURE_DIM, GCN_HIDDEN_DIM1)\n",
    "        self.ln1 = nn.LayerNorm(GCN_HIDDEN_DIM1)\n",
    "        self.gc2 = GraphConvolution(GCN_HIDDEN_DIM1, GCN_HIDDEN_DIM2)  # Additional hidden layer\n",
    "        self.ln2 = nn.LayerNorm(GCN_HIDDEN_DIM2)\n",
    "        self.gc3 = GraphConvolution(GCN_HIDDEN_DIM2, GCN_OUTPUT_DIM)\n",
    "        self.ln3 = nn.LayerNorm(GCN_OUTPUT_DIM)\n",
    "        self.relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):  \n",
    "        x = self.gc1(x, adj)\n",
    "        x = self.relu1(self.ln1(x))\n",
    "        x = self.gc2(x, adj)  # Additional hidden layer\n",
    "        x = self.relu2(self.ln2(x))\n",
    "        x = self.gc3(x, adj)\n",
    "        output = self.relu3(self.ln3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f936abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN1, self).__init__()\n",
    "        self.gc1 = GraphConvolution(35, GCN_HIDDEN_DIM)\n",
    "        self.ln1 = nn.LayerNorm(GCN_HIDDEN_DIM)\n",
    "        self.gc2 = GraphConvolution(GCN_HIDDEN_DIM, GCN_OUTPUT_DIM)\n",
    "        self.ln2 = nn.LayerNorm(GCN_OUTPUT_DIM)\n",
    "        self.relu1 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu2 = nn.LeakyReLU(0.2,inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t# x.shape = (seq_len, GCN_FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = self.gc1(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu1(self.ln1(x))\n",
    "        x = self.gc2(x, adj)\n",
    "        output = self.relu2(self.ln2(x))\t# output.shape = (seq_len, GCN_OUTPUT_DIM)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9b1737d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, dense_dim, n_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.dense_dim)\n",
    "        self.fc2 = nn.Linear(self.dense_dim, self.n_heads)\n",
    "\n",
    "    def softmax(self, input, axis=1):\n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size) - 1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = torch.softmax(input_2d, dim=1)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size) - 1)\n",
    "\n",
    "    def forward(self, input):  \t\t\t\t# input.shape = (1, seq_len, input_dim)\n",
    "        x = torch.tanh(self.fc1(input))  \t# x.shape = (1, seq_len, dense_dim)\n",
    "        x = self.fc2(x)  \t\t\t\t\t# x.shape = (1, seq_len, attention_hops)\n",
    "        x = self.softmax(x, 1)\n",
    "        attention = x.transpose(1, 2)  \t\t# attention.shape = (1, attention_hops, seq_len)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a7a3b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # Calculate the number of input channels for the 1x1 convolution dynamically\n",
    "        self.conv1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.in_channels = in_channels if stride == 1 else out_channels\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Adjust input channels dynamically based on the output channels of the previous layer\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1x1(x)  # 1x1 convolution to adjust input channels\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "089cad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W.data)\n",
    "        nn.init.xavier_uniform_(self.a.data)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = torch.matmul(x, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(2), negative_slope=self.alpha)\n",
    "        \n",
    "        # Apply mask to the adjacency matrix\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        return F.elu(h_prime)\n",
    "\n",
    "class ModelS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelS, self).__init__()\n",
    "        self.att = GraphAttentionLayer(71, 64, dropout=0.2)\n",
    "        self.out_att = GraphAttentionLayer(64, 64, dropout=0.2)\n",
    "        self.attention = Attention(64, DENSE_DIM, ATTENTION_HEADS)\n",
    "\n",
    "        #self.fc_final = nn.Linear(hidden_size, num_classes)\n",
    "        self.fc_final = nn.Linear(72, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    def forward(self, x_W, adj_W, x_M, adj_M, A_W, A_M):\n",
    "        A_W = A_W.float()\n",
    "        A_M = A_M.float()\n",
    "        x_W = x_W.float()\n",
    "        x_M = x_M.float()\n",
    "\n",
    "\n",
    "        #for att in self.attentions:\n",
    "        x_M = F.elu(self.att(x_M, adj_M))\n",
    "        x_W = F.elu(self.att(x_W, adj_W))\n",
    "\n",
    "        x_M = F.elu(self.out_att(x_M, adj_M))\n",
    "        x_W = F.elu(self.out_att(x_W, adj_W)) \n",
    "        A_M=torch.cat((x_M,A_M),1)\n",
    "        A_W=torch.cat((x_W,A_W),1)\n",
    "        A_W = A_W.unsqueeze(0).float() \n",
    "        A_M = A_M.unsqueeze(0).float()\n",
    "        tot = A_M-A_W\n",
    "        #print(tot.shape)\n",
    "       # att1 = self.attention(A_M)  # att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "       # node_feature_embedding_avg = torch.bmm(att1, A_M.transpose(1, 2)).mean(dim=1)\n",
    "        #att2 = self.attention(A_W)  # att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        #node_feature_embedding_avg1 = torch.bmm(att2, A_W.transpose(1, 2)).mean(dim=1)\n",
    "        #node_feature_embedding_avg2 = node_feature_embedding_avg - node_feature_embedding_avg1\n",
    "        output = torch.sigmoid(self.fc_final(tot.mean(dim=1)))  # output.shape = (1, NUM_CLASSES)\n",
    "        return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6978e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        nn.init.xavier_uniform_(self.W.data)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = torch.matmul(x, self.W)\n",
    "        N = h.size(0)\n",
    "\n",
    "        aggregate_neighborhood = torch.matmul(adj, h)\n",
    "        h_concat = torch.cat([h, aggregate_neighborhood], dim=1)\n",
    "\n",
    "        return F.elu(h_concat)\n",
    "    \n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "class ModelS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelS, self).__init__()\n",
    "        self.sage1 = GraphSAGELayer(114, 32)\n",
    "        self.sage2 = GraphSAGELayer(64 , 64)\n",
    "        self.resnet_layer = ResNetBlock(128, 128)  # Adjust num_blocks as needed\n",
    "        self.fc_final = nn.Linear(128, 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x_W, adj_W, x_M, adj_M, A_W, A_M):\n",
    "        A_W = A_W.float()\n",
    "        A_M = A_M.float()\n",
    "        x_W = A_W.float()\n",
    "        x_M = A_M.float()\n",
    "        A_M = F.elu(self.sage1(A_M, adj_M))\n",
    "        A_W = F.elu(self.sage1(A_W, adj_W))\n",
    "\n",
    "        A_M = F.elu(self.sage2(A_M, adj_M))\n",
    "        A_W = F.elu(self.sage2(A_W, adj_W))\n",
    "\n",
    "        A_W = A_W.unsqueeze(0).float()\n",
    "        A_M = A_M.unsqueeze(0).float()\n",
    "        tot = A_M - A_W\n",
    "        tot = tot.permute(0, 2, 1)  # Reshape for 1D convolution\n",
    "        tot = self.resnet_layer(tot)\n",
    "\n",
    "        # Pooling along the sequence dimension\n",
    "        tot = F.avg_pool1d(tot, kernel_size=tot.size(2))\n",
    "\n",
    "        # Reshape for fully connected layer\n",
    "        tot = tot.view(tot.size(0), -1)\n",
    "        output = torch.sigmoid(self.fc_final(tot))  # output.shape = (1, NUM_CLASSES)\n",
    "        return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b89f2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "       # self.GNN = ComplexGraphNeuralNetwork(input_features, hidden_features, output_features, num_layers=3)\n",
    "        self.gcn1 = GCN1()\n",
    "        self.gcn = GCN()\n",
    "       # self.resnet = ResBlock(GCN_OUTPUT_DIM,64,64)\n",
    "        self.attention = Attention(64, DENSE_DIM, ATTENTION_HEADS)\n",
    "        self.fc_final = nn.Linear(64, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x_W, adj_W,x_M,adj_M,A_W,A_M):  \n",
    "        x_W = x_W.float()\n",
    "        A_W = A_W.float()\n",
    "        #x_W=torch.cat((x_W,A_W),1)\n",
    "       # print(x_W.shape)\n",
    "        x_W = self.gcn(x_W, adj_W)\n",
    "        A_W = self.gcn1(A_W,adj_W)\n",
    "       # print(x_W.shape)# x.shape = (seq_len, GAT_OUTPUT_DIM)\n",
    "        x_M = x_M.float()\n",
    "        A_M = A_M.float()\n",
    "        #x_M=torch.cat((x_M,A_M),1)        \n",
    "        x_M = self.gcn(x_M, adj_M)  \n",
    "        A_M = self.gcn1(A_M,adj_M)\n",
    "        #A_M = self.gcn1(A_M,adj_M)\n",
    "       # print(A_M.shape,x_M.shape)\n",
    "        #x_M=torch.cat((x_M,A_M),1)\n",
    "        #x_W=torch.cat((x_W,A_W),1)\n",
    "        x_W = x_W + A_W\n",
    "        x_M = x_M + A_M\n",
    "        #print(x_M.shape)\n",
    "        #x_M = x_M.sum(1)\n",
    "        #x_W = x_W.sum(1)\n",
    "       # print(x_M.shape)\n",
    "        #tot = x_W-x_M\n",
    "        x_W = x_W.unsqueeze(0).float() \n",
    "        x_M = x_M.unsqueeze(0).float()\n",
    "        #print(tot.shape)\n",
    "        #tot = x_W-x_M\n",
    "        #print(tot.shape)\n",
    "        att1 = self.attention(x_M) # att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        #print(att.shape)\n",
    "        #print(tot.shape)\n",
    "       # assert tot.size(2) == att.size(2),\n",
    "        node_feature_embedding =  att1 @ x_M \n",
    "        node_feature_embedding_avg = torch.sum(node_feature_embedding,1) / self.attention.n_heads \n",
    "        att2 = self.attention(x_W) # att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        #print(att.shape)\n",
    "        #print(tot.shape)\n",
    "       # assert tot.size(2) == att.size(2),\n",
    "        node_feature_embedding1 =  att2 @ x_W \n",
    "        node_feature_embedding_avg1 = torch.sum(node_feature_embedding1,\n",
    "                                               1) / self.attention.n_heads \n",
    "        node_feature_embedding_avg2 = node_feature_embedding_avg1 - node_feature_embedding_avg\n",
    "        output =  torch.sigmoid(self.fc_final(node_feature_embedding_avg2))  \t# output.shape = (1, NUM_CLASSES)\n",
    "        return output.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "92937142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, epoch):\n",
    "\n",
    "    epoch_loss_train = 0.0\n",
    "    n_batches = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        model.optimizer.zero_grad()\n",
    "        sequence_feature, sequence_feature_wild,sequence_graph , sequence_graph_wild,A,Aw, labels, sequence_names = data\n",
    "\n",
    "        sequence_feature = torch.squeeze(sequence_feature)\n",
    "        sequence_graph = torch.squeeze(sequence_graph)\n",
    "        A = torch.squeeze(A)\n",
    "        sequence_feature_wild = torch.squeeze(sequence_feature_wild)\n",
    "        sequence_graph_wild = torch.squeeze(sequence_graph_wild)\n",
    "        Aw = torch.squeeze(Aw)\n",
    "       # if torch.cuda.is_available():\n",
    "       #     features = Variable(sequence_feature.cuda())\n",
    "       #     graphs = Variable(sequence_graph.cuda())\n",
    "        #    features_wild = Variable(sequence_feature_wild.cuda())\n",
    "        #    graphs_wild = Variable(sequence_graph_wild.cuda())\n",
    "         #   A = Variable(A.cuda())\n",
    "         #   Aw = Variable(Aw.cuda())\n",
    "         #   y_true = Variable(labels.cuda())\n",
    "        #else:\n",
    "        features = Variable(sequence_feature)\n",
    "        graphs = Variable(sequence_graph)\n",
    "        features_wild = Variable(sequence_feature_wild)\n",
    "        graphs_wild = Variable(sequence_graph_wild)\n",
    "        A = Variable(A)\n",
    "        Aw = Variable(Aw)\n",
    "        y_true = Variable(labels)\n",
    "\n",
    "        y_pred = model(features_wild, graphs_wild,features, graphs,Aw,A)\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        # calculate loss\n",
    "        loss = model.criterion(y_pred, y_true)\n",
    "        #l2_lambda = 0.001\n",
    "        #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        #loss = loss + l2_lambda * l2_norm\n",
    "        #print(loss)\n",
    "\n",
    "        # backward gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # update all parameters\n",
    "        model.optimizer.step()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    epoch_loss_train_avg = epoch_loss_train / n_batches\n",
    "    return epoch_loss_train_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "02cfae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    valid_pred = []\n",
    "    valid_true = []\n",
    "    valid_name = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            sequence_feature, sequence_feature_wild,sequence_graph , sequence_graph_wild,A,Aw, labels, sequence_names = data\n",
    "\n",
    "            sequence_feature = torch.squeeze(sequence_feature)\n",
    "            sequence_graph = torch.squeeze(sequence_graph)\n",
    "            A= torch.squeeze(A)\n",
    "            sequence_feature_wild = torch.squeeze(sequence_feature_wild)\n",
    "            sequence_graph_wild = torch.squeeze(sequence_graph_wild)\n",
    "            Aw = torch.squeeze(Aw)\n",
    "           # if torch.cuda.is_available():\n",
    "            #    features = Variable(sequence_feature.cuda())\n",
    "             #   graphs = Variable(sequence_graph.cuda())\n",
    "              #  features_wild = Variable(sequence_feature_wild.cuda())\n",
    "               # graphs_wild = Variable(sequence_graph_wild.cuda())\n",
    "               # A = Variable(A.cuda())\n",
    "               # Aw = Variable(Aw.cuda())\n",
    "               # y_true = Variable(labels.cuda())\n",
    "            #else:\n",
    "            features = Variable(sequence_feature)\n",
    "            graphs = Variable(sequence_graph)\n",
    "            features_wild = Variable(sequence_feature_wild)\n",
    "            graphs_wild = Variable(sequence_graph_wild)\n",
    "            A = Variable(A)\n",
    "            Aw = Variable(Aw)\n",
    "            y_true = Variable(labels)\n",
    "\n",
    "            y_pred = model(features_wild, graphs_wild,features, graphs,Aw,A)\n",
    "            y_true = y_true.float()\n",
    "\n",
    "            loss = model.criterion(y_pred, y_true)\n",
    "            #l2_lambda = 0.001\n",
    "            #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            #loss = loss + l2_lambda * l2_norm\n",
    "            #print(loss)\n",
    "                \n",
    "            y_pred = y_pred.cpu().detach().numpy().tolist()\n",
    "            y_true = y_true.cpu().detach().numpy().tolist()\n",
    "            valid_pred.extend(y_pred)\n",
    "            valid_true.extend(y_true)\n",
    "            valid_name.extend(sequence_names)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    epoch_loss_avg = epoch_loss / n_batches\n",
    "\n",
    "    return epoch_loss_avg, valid_true, valid_pred, valid_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ccdf1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataframe,valid_dataframe,fold=0):\n",
    "    train_loader = DataLoader(dataset=Dataset(df[:600]) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(dataset=Dataset(df[:300]) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "\n",
    "    train_losses = []\n",
    "    train_pearson = []\n",
    "    train_r2 = []\n",
    "\n",
    "    valid_losses = []\n",
    "    valid_pearson = []\n",
    "    valid_r2 = []\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(NUMBER_EPOCHS):\n",
    "        print(\"\\n========== Train epoch \" + str(epoch + 1) + \" ==========\")\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss_train_avg = train_one_epoch(model, train_loader, epoch + 1)\n",
    "        #print(\"========== Evaluate Train set ==========\")\n",
    "        #_, train_true, train_pred, _ = evaluate(model, train_loader)\n",
    "       # print(train_pred)\n",
    "        #result_train = analysis(train_true, train_pred)\n",
    "        #print(\"Train loss: \", np.sqrt(epoch_loss_train_avg))\n",
    "        #print(\"Train pearson:\", result_train['pearson'])\n",
    "        #print(\"Train r2:\", result_train['r2'])\n",
    "\n",
    "        #train_losses.append(np.sqrt(epoch_loss_train_avg))\n",
    "        #train_pearson.append(result_train['pearson'])\n",
    "        #train_r2.append(result_train['r2'])\n",
    "\n",
    "        print(\"========== Evaluate Valid set ==========\")\n",
    "        epoch_loss_valid_avg, valid_true, valid_pred, valid_name = evaluate(model, valid_loader)\n",
    "        result_valid = analysis(valid_true, valid_pred)\n",
    "        print(\"Valid loss: \", np.sqrt(epoch_loss_valid_avg))\n",
    "        print(\"Valid pearson:\", result_valid['pearson'])\n",
    "        print(\"Valid r2:\", result_valid['r2'])\n",
    "\n",
    "\n",
    "        valid_losses.append(np.sqrt(epoch_loss_valid_avg))\n",
    "        valid_pearson.append(result_valid['pearson'])\n",
    "        valid_r2.append(result_valid['r2'])\n",
    "\n",
    "        if best_val_loss > epoch_loss_valid_avg:\n",
    "            best_val_loss = epoch_loss_valid_avg\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), os.path.join(Model_Path, 'Fold' + str(fold) + '_best_model.pkl'))\n",
    "            valid_detail_dataframe = pd.DataFrame({'names': valid_name, 'stability': valid_true, 'prediction': valid_pred})\n",
    "            valid_detail_dataframe.sort_values(by=['names'], inplace=True)\n",
    "            valid_detail_dataframe.to_csv(Result_Path + 'Fold' + str(fold) + \"_valid_detail.csv\", header=True, sep=',')\n",
    " \n",
    "    result_all = {\n",
    "        'Train_loss': train_losses,\n",
    "        'Train_pearson': train_pearson,\n",
    "        'Train_r2': train_r2,\n",
    "        'Valid_loss': valid_losses,\n",
    "        'Valid_pearson': valid_pearson,\n",
    "        'Valid_r2': valid_r2,\n",
    "        'Best_epoch': [best_epoch for _ in range(len(train_losses))]\n",
    "    }\n",
    "    result = pd.DataFrame(result_all)\n",
    "    print(\"Fold\", str(fold), \"Best epoch at\", str(best_epoch))\n",
    "    result.to_csv('result.csv')\n",
    "\n",
    "def analysis(y_true, y_pred):\n",
    "\n",
    "    # continous evaluate\n",
    "    pearson = np.corrcoef(y_true, y_pred)[0,1]\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    result = {\n",
    "        'pearson': pearson,\n",
    "        'r2': r2\n",
    "    }\n",
    "    return result\n",
    "def cross_validation(all_dataframe,fold_number=10):\n",
    "    print(\"split_seed: \", SEED)\n",
    "    sequence_names = all_dataframe['names'].values\n",
    "    sequence_labels = all_dataframe['stability'].values\n",
    "    kfold = KFold(n_splits=fold_number, shuffle=True)\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, valid_index in kfold.split(sequence_names, sequence_labels):\n",
    "        print(\"\\n========== Fold \" + str(fold + 1) + \" ==========\")\n",
    "        train_dataframe = all_dataframe.iloc[train_index, :]\n",
    "        valid_dataframe = all_dataframe.iloc[valid_index, :]\n",
    "        print(\"Training on\", str(train_dataframe.shape[0]), \"examples, Validation on\", str(valid_dataframe.shape[0]),\n",
    "              \"examples\")\n",
    "        model = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "\n",
    "        train(model, train_dataframe, valid_dataframe, fold + 1)\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Train epoch 1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 139.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 424.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.1871978089578763\n",
      "Valid pearson: 0.33933320098355085\n",
      "Valid r2: -0.06815870664105828\n",
      "\n",
      "========== Train epoch 2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 141.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 423.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.1896577296011293\n",
      "Valid pearson: 0.2917994938873823\n",
      "Valid r2: -0.08295371301083043\n",
      "\n",
      "========== Train epoch 3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 140.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 430.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.1885504334728514\n",
      "Valid pearson: 0.28391544591089846\n",
      "Valid r2: -0.0768066682668953\n",
      "\n",
      "========== Train epoch 4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 132.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 373.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.188007534637785\n",
      "Valid pearson: 0.30470239051202647\n",
      "Valid r2: -0.07342102130389816\n",
      "\n",
      "========== Train epoch 5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 135.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 450.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.1876517290925062\n",
      "Valid pearson: 0.22205809162174317\n",
      "Valid r2: -0.0712728473203843\n",
      "\n",
      "========== Train epoch 6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:04<00:00, 137.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 426.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss:  1.1875754361703266\n",
      "Valid pearson: 0.2402919825976987\n",
      "Valid r2: -0.0718838213267241\n",
      "\n",
      "========== Train epoch 7 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|██████████████████████████████████████▍                                        | 292/600 [00:01<00:02, 147.47it/s]"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "#if torch.cuda.is_available():\n",
    " #   model.cuda()\n",
    "train(model,df,df[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "520b2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Alternatively, you can specify a GPU device by index\n",
    "# device = torch.device(\"cuda:1\")  # Use the second GPU\n",
    "\n",
    "# Print the selected device\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc45c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataframe):\n",
    "    test_loader = DataLoader(dataset=Dataset(ds) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "    test_result = {}\n",
    "\n",
    "    for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model = Model()\n",
    "        #if torch.cuda.is_available():\n",
    "        #    model.cuda()\n",
    "        model.load_state_dict(torch.load(Model_Path + model_name,map_location='cpu'))\n",
    "\n",
    "        epoch_loss_test_avg, test_true, test_pred, test_name = evaluate(model, test_loader)\n",
    "        result_test = analysis(test_true, test_pred)\n",
    "        print(\"\\n========== Evaluate Test set ==========\")\n",
    "        print(\"Test loss: \", np.sqrt(epoch_loss_test_avg))\n",
    "        print(\"Test pearson:\", result_test['pearson'])\n",
    "        print(\"Test r2:\", result_test['r2'])\n",
    "\n",
    "        test_result[model_name] = [\n",
    "            np.sqrt(epoch_loss_test_avg),\n",
    "            result_test['pearson'],\n",
    "            result_test['r2'],\n",
    "        ]\n",
    "\n",
    "        test_detail_dataframe = pd.DataFrame({'names': test_name, 'stability': test_true, 'prediction': test_pred})\n",
    "        test_detail_dataframe.sort_values(by=['names'], inplace=True)\n",
    "        test_detail_dataframe.to_csv(Result_Path + model_name + \"_test_detail.csv\", header=True, sep=',')\n",
    "\n",
    "    test_result_dataframe = pd.DataFrame.from_dict(test_result, orient='index',\n",
    "                                                   columns=['loss', 'pearson', 'r2', 'precision'])\n",
    "    test_result_dataframe.to_csv(Result_Path + \"test_result.csv\", index=True, header=True, sep=',')\n",
    "\n",
    "\n",
    "def analysis(y_true, y_pred):\n",
    "\n",
    "    # continous evaluate\n",
    "    pearson = pearsonr(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    result = {\n",
    "        'pearson': pearson,\n",
    "        'r2': r2\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49db916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold2_best_model.pkl\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test(df)\n",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(test_dataframe)\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#if torch.cuda.is_available():\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#    model.cuda()\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(Model_Path \u001b[38;5;241m+\u001b[39m model_name,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m epoch_loss_test_avg, test_true, test_pred, test_name \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[0;32m     13\u001b[0m result_test \u001b[38;5;241m=\u001b[39m analysis(test_true, test_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72])."
     ]
    }
   ],
   "source": [
    "test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "314ea7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\target\\train.py\", line 434, in <module>\n",
      "    train(df,0)\n",
      "  File \"D:\\target\\train.py\", line 370, in train\n",
      "    train_loader = DataLoader(dataset=ProDataset(train_dataframe) ,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\target\\train.py\", line 56, in __init__\n",
      "    self.names = dataframe['name'].values\n",
      "                 ~~~~~~~~~^^^^^^^^\n",
      "TypeError: 'int' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "!python train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "6655c916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>names</th>\n",
       "      <th>stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1A23A_C30S</td>\n",
       "      <td>-1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1A23A_C33S</td>\n",
       "      <td>-1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1A23A_H32L</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1A23A_H32S</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1A23A_H32Y</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       names  stability\n",
       "0           0  1A23A_C30S      -1.80\n",
       "1           1  1A23A_C33S      -1.10\n",
       "2           2  1A23A_H32L       4.95\n",
       "3           3  1A23A_H32S       4.40\n",
       "4           4  1A23A_H32Y       4.63"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65724adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold2_best_model.pkl\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_name)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(Model_Path \u001b[38;5;241m+\u001b[39m model_name,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72])."
     ]
    }
   ],
   "source": [
    "for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model = Model()\n",
    "        model.load_state_dict(torch.load(Model_Path + model_name,map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e91689bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(Model_Path \u001b[38;5;241m+\u001b[39m model_name,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"GNN.conv1.weight\", \"GNN.conv1.bias\", \"GNN.convs.0.weight\", \"GNN.convs.0.bias\", \"GNN.convs.1.weight\", \"GNN.convs.1.bias\", \"GNN.conv_final.weight\", \"GNN.conv_final.bias\", \"GNN.fc1.weight\", \"GNN.fc1.bias\", \"GNN.fc2.weight\", \"GNN.fc2.bias\". \n\tUnexpected key(s) in state_dict: \"gcn1.gc11.weight\", \"gcn1.gc11.bias\", \"gcn1.ln11.weight\", \"gcn1.ln11.bias\", \"gcn1.gc21.weight\", \"gcn1.gc21.bias\", \"gcn1.ln21.weight\", \"gcn1.ln21.bias\", \"gcn1.gc31.weight\", \"gcn1.gc31.bias\", \"gcn1.ln31.weight\", \"gcn1.ln31.bias\", \"FC1.weight\", \"FC1.bias\", \"FC2.weight\", \"FC2.bias\", \"FC3.weight\", \"FC3.bias\". \n\tsize mismatch for gcn.gc3.weight: copying a param with shape torch.Size([256, 71]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for gcn.gc3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.weight: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for gcn.ln3.bias: copying a param with shape torch.Size([71]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for attention.fc1.weight: copying a param with shape torch.Size([16, 79]) from checkpoint, the shape in current model is torch.Size([16, 72]).\n\tsize mismatch for fc_final.weight: copying a param with shape torch.Size([1, 99]) from checkpoint, the shape in current model is torch.Size([1, 72])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(Model_Path + model_name,map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "078f6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN ComplexGraphNeuralNetwork(\n",
      "  (conv1): GraphConvolution (71 -> 64)\n",
      "  (convs): ModuleList(\n",
      "    (0-1): 2 x GraphConvolution (64 -> 64)\n",
      "  )\n",
      "  (conv_final): GraphConvolution (64 -> 64)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=704, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      ")\n",
      "gcn GCN(\n",
      "  (gc1): GraphConvolution (71 -> 256)\n",
      "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (gc2): GraphConvolution (256 -> 128)\n",
      "  (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (gc3): GraphConvolution (128 -> 64)\n",
      "  (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (relu3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      ")\n",
      "attention Attention(\n",
      "  (fc1): Linear(in_features=72, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n",
      "fc_final Linear(in_features=72, out_features=1, bias=True)\n",
      "criterion MSELoss()\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8178f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
